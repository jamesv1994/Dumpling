name: Deploy Apache Airflow DAGs
on:
  push:
    branches: [dev]

  # Manual workflow execution for testing
  workflow_dispatch:

jobs:
  deploy_airflow_dags:
    runs-on: [self-hosted, "apache-airflow-{{ GITHUB_REF }}"]
    env: 
      GITHUB_HASH_LOCATION: /appdata/hashes/airflow_dags/additional_files_dev
      TEMP_LOCATION_CHANGED_FILES: /tmp/airflow_depl/
      ANSIBLE_CHECKOUT_LOCATION: /appdata/github/n2125168/automation/


    steps:
      # Get the latest revision
      - uses: actions/checkout@v2
      
      # # Exeuction of _schema_db2_deploy.yml Ansible playbook, so that we always have the correct python scripts deployed
      # - name: Execute the _schema_db2_deploy.yml  
      #   run: "/appdata/python_venv/pkg_management_v0.1/bin/activate && cd /appdata/github/automation/"

      - name: "Copy over dags to temp-area"
        run: |
          find $GITHUB_WORKSPACE/dags/ -maxdepth 1 -type f -exec cp {} ${{ env.TEMP_LOCATION_CHANGED_FILES }} \;

      - name: Deploy DAG Files onto server using Ansible Playbook
        run: |
          environment=$(echo $GITHUB_REF | awk -F'/' '{print $NF}');
          echo "Deploying on environment: ${environment}"
          # For now hardcoding to DEV
          # echo "activating Ansible VENV"; 
          source /appdata/python_venv/ansible-runner/bin/activate
          echo "Executing Playbook";
          ansible-playbook $ANSIBLE_CHECKOUT_LOCATION/airflow_deploy_dags_github_runner.yml -i $ANSIBLE_CHECKOUT_LOCATION/inventories/server.yaml -e "dag_src=$TEMP_LOCATION_CHANGED_FILES/ airflow_environment=$environment";

      # Make sure, that no other location will be removed!
      - name: "Clear out the depl temp dir"
        run: "rm -rf $TEMP_LOCATION_CHANGED_FILES/*"
        if: ${{ env.TEMP_LOCATION_CHANGED_FILES == '/tmp/airflow_depl/' }}
